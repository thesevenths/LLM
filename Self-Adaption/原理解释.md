一、婴儿刚出生，什么都不懂，面对新鲜事物，怎么学习了？

* 婴儿面对新事物：一个碗，不知道是什么，怎么办？只能探索：用眼看、用鼻闻、用手摸、用牙咬，用力摔.....
* 大人看见后，告诉婴儿：可以看，可以摸，但是不能咬，不能摔，也没必要闻.....
* 婴儿经过大人的指导，后续再看见碗，只会看、摸，不会再乱咬、摔打，也不会再闻了......

整个过程婴儿都是自主学习，只需要大人判断什么能做、什么不能做就行了！整个过程是不是和sft、RL很像了？

二、现阶段，做RL时，**免不了人工标注数据，比如数学题要结果，编程题也要结果，标注多了成本就高啊**！怎么降低标注成本？怎么让模型自己迭代训练了？

2.1   传统方案：model自己生成大量数据，人工/其他LLM筛选出质量高的数据然后再微调；

* 缺陷：因为是“自产自销”，经过多轮迭代后，数据多样性会严重降低，导致model collapse

2.2   self-edit 方案：

    既然上述方式会导致数据多样性严重降低，那怎么提升自产数据的多样性了？

* 人类准备少量问题，比如100个（尽量选model pretrain时没有的主题）
* model**生成多个、多样的回答，比如5个QA对（相当于人类婴儿探索新事物）；同时生成需要训练的epoch数、lr等train 参数，也就是让model执行决定finetune的方案**！
  * 让数据尽量多样化，增加generalization能力
  * 模型不仅生成“新的训练样本”，还生成“训练这批样本我用多少 epoch／哪种增强／哪种超参 (learning rate etc)”等。也就是说，**训练的流程（meta-training）也在模型控制之内**
  * 这既是原论文所谓的self-edit
* 用上述自己生成的数据做sft，update weight！

至此，finetune是不是已经结束了？显然不是，可能还有如下问题：

* model自己生成的答案是不是对的？没有人工检查，谁能确保答案一定正确了？

2.3  downstream task 验证

    上一步生成的candidated self-edit答案有对有错，用来sft的效果好不好了？是骡子是马，拖出去遛一遛不就知道了？直接来硬的：**用下游任务的回答来判断上一步finetune的效果，并使用结果作为reward信号（R，self edit），继续RL,  用on-policy的策略！**

    通过 supervised finetuning (SFT) + RL outer-loop，使得模型能“持久地”适应新知识／新任务

    总结：**不只是简单粗暴地自动生成 synthetic data，而是：模型“学习如何为自己生成最有用的训练数据+训练策略”**。

* 给模型一段**新的事实**性文本，让**模型吸收该知识 (finetune后能够回答相关问题)**; 模型通过 self-edit 生成的“合成训练数据 + 微调”表现**优于直接用原始文本微调**、也**优于用 GPT-4 生成数据微调的 baselines**
* 少样本泛化 (few-shot generalization)任务上做实验：模型从**少量示例学习一个新任务，通过自己选择数据增强+训练超参，从而获得比传统 few-shot 更好的泛化**。
  * 说明self adaption在特定场景下是有效的

2.4 总结

paper的动机：LLMs are powerful but static;… they lack mechanisms to **adapt their weights in response to new tasks, knowledge, or examples**

* SEAL 提出了一个方向，向 lifelong learning 靠拢

3、其他类似不需要大量人工标注的paper：



| 方案名称                                                               | 数据来源与优质保证（如何生成优质数据，保证多样性）                                                                                                                            | 判断finetune效果（如何评估输出正确性，无人工）                                   | 使用场景                             | 优点                               | 缺点                           |
| ---------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | ------------------------------------ | ---------------------------------- | ------------------------------ |
| SEAL<br />(Self-Adapting Language Models, 2025)                        | 从无标签输入（如新事实文本）生成self-edits（合成QA对+训练策略），<br />优质通过RL过滤高reward edit；多样性用变异工具调用（如数据增强）<br />和ReST^EM on-policy训练避免崩溃。 | 下游任务性能作为reward<br />（e.g., QA准确率提升作为二元信号），RL外环迭代评估。 | 知识整合、few-shot泛化。             | 端到端自主、优于GPT-4基线。        | 计算开销高、遗忘风险。         |
| EVOL-RL<br /> (Evolving Language Models without Labels, 2025更新)      | 从无标签测试数据采样多响应，多数投票生成稳定（优质）数据；<br />多样性用新颖奖励（语义嵌入距离）促进变异，防止熵崩溃。                                                        | 相对优势（标准化优势 vs. 同组响应）作为reward，<br />pass@k指标评估泛化。        | 数学推理（如AIME、MATH）、跨域泛化。 | 平衡稳定与探索、显著提升pass@1/n。 | 计算密集、限于结构化任务。     |
| R-Zero<br /> (Self-Evolving Reasoning LLM from Zero Data, 2025)        | Challenger代理生成边缘问题，Solver代理解决，形成自举数据集；<br />优质通过互动过滤，多样性用代理co-evolve动态课程。                                                           | Solver成功率作为reward信号，<br />迭代前后性能对比（e.g., 伪标签准确率）。       | 推理任务、从零数据启动。             | 完全自主、鲁棒课程学习。           | 双代理开销、伪标签可靠性下降。 |
| Large Language Models Can Self-Improve (2025扩展)                      | 自生成高信心样本和理由，从预训练语料蒸馏；<br />优质用内在置信度过滤，多样性通过多次采样。                                                                                    | 自一致性或置信度作为代理reward，<br />下游基准对比。                             | 一般推理、数学任务。                 | 简单高效、零标签提升10-15%。       | 易模式崩溃、依赖基础模型。     |
| ALMA<br /> (Alignment with Minimal Annotation, 2025)                   | 用内在知识生成合成对齐数据（如偏好对）；<br />优质用预训练先验，多样性通过种子变异。                                                                                          | 自评偏好作为reward，迭代对齐分数。                                               | 对齐优化、偏好学习。                 | 成本低、易扩展。                   | 对齐不稳定、主观偏置。         |
| TTRL <br />(Test-Time Reinforcement Learning, 2025)                    | 从无标签测试数据采样推理路径；<br />优质用自一致性过滤，多样性通过探索性采样。                                                                                                | 预训练先验作为reward（e.g., 路径一致性），pass@1/n评估。                         | 在线推理优化、MATH任务。             | 在线适应、提升5-10%。              | 易认知崩溃、需改进。           |
| Self-Rewarding Language Models (2025)                                  | 自生成输出+reward；<br />优质用模型判断，多样性通过多步采样。                                                                                                                 | 模型作为meta-judge自评质量，RL迭代。                                             | 多步推理、自对齐。                   | 完全自主、优于无reward。           | Reward主观、偏置风险。         |
| ICM<br /> (Internal Coherence Maximization, 2025)                      | 从内部表示生成一致样本；<br />优质用一致性过滤，多样性通过最大化内在变异。                                                                                                    | 内部一致性作为reward，性能回归评估。                                             | 鲁棒性提升、一般微调。               | 稳定、易实现。                     | 泛化有限、reward单一。         |
| ALAS <br />(Autonomous Learning Agent System, 2025)                    | 代理管道自生成检索-生成循环；<br />优质用执行结果过滤，多样性通过多代理互动。                                                                                                 | 执行成功率作为自然reward，迭代基准。                                             | 终身学习、新领域适应。               | 灵活、多步交互。                   | 代理复杂、设计难度。           |
| ACE <br />(Evolving Contexts for Self-Improving Language Models, 2025) | 代理进化上下文生成数据；<br />优质用反馈循环，多样性通过环境变异。                                                                                                            | 任务成功率作为自反馈。                                                           | 新环境适应（如AppWorld）。           | 实用、迭代快速。                   | 需模拟环境、反馈依赖。         |
| Autonomous Learning for Domain Adaptation (2025)                       | 在无标签领域数据上自创建改进样本；<br />优质用领域信号，多样性通过适应变异。                                                                                                  | 内在领域一致性作为reward。                                                       | 跨领域转移。                         | 针对性强、零人类。                 | 领域依赖、初始化挑战。         |
| Self-Improving Alignment with LLM-as-a-Meta-Judge (2025)               | 自生成判断样本；<br />优质用元级过滤，<br />多样性通过meta-rewarding变异。                                                                                                    | Meta-judging自炼化，迭代判断分数。                                               | 判断改进、对齐迭代。                 | 元级优化、深度适应。               | 计算重、循环偏置。             |
| MIT Self-Trained Entailment Models (2025)                              | 自生成蕴涵标签；<br />优质用生成概率过滤，<br />多样性通过任务变体。                                                                                                          | 自蕴涵一致性作为reward。                                                         | 逻辑推理、模型扩展。                 | 高效、可扩展。                     | 任务特定、泛化需调优。         |
| DaaR<br /> (Diversity as a Reward, 2025新)                             | 用嵌入空间生成伪域质心，合成数据；<br />优质用语义熵，多样性通过几何约束自监督。                                                                                              | 多样性分数作为reward，跨基准平均性能。                                           | 域不确定数据微调。                   | SOTA效率、70%低GPU。               | 嵌入依赖、需冻结层。           |
| Evol-Instruct (2025扩展)                                               | 从种子指令in-depth/in-breadth进化数据；<br />优质用复杂化过滤，多样性通过突变提示。                                                                                           | 自验证和FOBAR作为reward，数学任务准确率。                                        | 指令跟随、数学任务。                 | 从少种子生成多样、提升数学能力。   | 进化深度有限、计算多。         |
