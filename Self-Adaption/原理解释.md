一、婴儿刚出生，什么都不懂，面对新鲜事物，怎么学习了？

* 婴儿面对新事物：一个碗，不知道是什么，怎么办？只能探索：用眼看、用鼻闻、用手摸、用牙咬，用力摔.....
* 大人看见后，告诉婴儿：可以看，可以摸，但是不能咬，不能摔，也没必要闻.....
* 婴儿经过大人的指导，后续再看见碗，只会看、摸，不会再乱咬、摔打，也不会再闻了......

整个过程婴儿都是自主学习，只需要大人判断什么能做、什么不能做就行了！整个过程是不是和sft、RL很像了？



二、现阶段，做RL时，**免不了人工标注数据，比如数学题要结果，编程题也要结果，标注多了成本就高啊**！怎么降低标注成本？怎么让模型自己迭代训练了？

2.1   传统方案：model自己生成大量数据，人工/其他LLM筛选出质量高的数据然后再微调；

* 缺陷：因为是“自产自销”，经过多轮迭代后，数据多样性会严重降低，导致model collapse

2.2   self-edit 方案：

    既然上述方式会导致数据多样性严重降低，那怎么提升自产数据的多样性了？

* 人类准备少量问题，比如100个（尽量选model pretrain时没有的主题）
* model**生成多个、多样的回答，比如5个QA对（相当于人类婴儿探索新事物）；同时生成需要训练的epoch数、lr等train 参数，也就是让model执行决定finetune的方案**！
  * 让数据尽量多样化，增加generalization能力
  * 模型不仅生成“新的训练样本”，还生成“训练这批样本我用多少 epoch／哪种增强／哪种超参 (learning rate etc)”等。也就是说，**训练的流程（meta-training）也在模型控制之内**
  * 这既是原论文所谓的self-edit
* 用上述自己生成的数据做sft，update weight！

至此，finetune是不是已经结束了？显然不是，可能还有如下问题：

* model自己生成的答案是不是对的？没有人工检查，谁能确保答案一定正确了？

2.3  downstream task 验证

    上一步生成的candidated self-edit答案有对有错，用来sft的效果好不好了？是骡子是马，拖出去遛一遛不就知道了？直接来硬的：**用下游任务的回答来判断上一步finetune的效果，并使用结果作为reward信号（R，self edit），继续RL,  用on-policy的策略！**

    通过 supervised finetuning (SFT) + RL outer-loop，使得模型能“持久地”适应新知识／新任务

    总结：**不只是简单粗暴地自动生成 synthetic data，而是：模型“学习如何为自己生成最有用的训练数据+训练策略”**。

* 给模型一段**新的事实**性文本，让**模型吸收该知识 (finetune后能够回答相关问题)**; 模型通过 self-edit 生成的“合成训练数据 + 微调”表现**优于直接用原始文本微调**、也**优于用 GPT-4 生成数据微调的 baselines**
* 少样本泛化 (few-shot generalization)任务上做实验：模型从**少量示例学习一个新任务，通过自己选择数据增强+训练超参，从而获得比传统 few-shot 更好的泛化**。
  * 说明self adaption在特定场景下是有效的

2.4 总结

* paper的动机：LLMs are powerful but static;… they lack mechanisms to **adapt their weights in response to new tasks, knowledge, or examples**
  * SEAL 提出了一个方向，向 lifelong learning 靠拢
