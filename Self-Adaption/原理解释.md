一、婴儿刚出生，什么都不懂，面对新鲜事物，怎么学习了？

* 婴儿面对新事物：一个碗，不知道是什么，怎么办？只能探索：用眼看、用鼻闻、用手摸、用牙咬，用力摔.....
* 大人看见后，告诉婴儿：可以看，可以摸，但是不能咬，不能摔，也没必要闻.....
* 婴儿经过大人的指导，后续再看见碗，只会看、摸，不会再乱咬、摔打，也不会再闻了......

整个过程婴儿都是自主学习，只需要大人判断什么能做、什么不能做就行了！整个过程是不是和sft、RL很像了？

二、现阶段，做RL时，**免不了人工标注数据，比如数学题要结果，编程题也要结果，标注多了成本就高啊**！怎么降低标注成本？怎么让模型自己迭代训练了？

2.1   传统方案：model自己生成大量数据，人工/其他LLM筛选出质量高的数据然后再微调；

* 缺陷：因为是“自产自销”，经过多轮迭代后，数据多样性会严重降低，导致model collapse

2.2   self-edit 方案：

    既然上述方式会导致数据多样性严重降低，那怎么提升自产数据的多样性了？

* 人类准备少量问题，比如100个（尽量选model pretrain时没有的主题）
* model**生成多个、多样的回答，比如5个QA对（相当于人类婴儿探索新事物）；同时生成需要训练的epoch数、lr等train 参数，也就是让model执行决定finetune的方案**！
  * 让数据尽量多样化，增加generalization能力
  * 模型不仅生成“新的训练样本”，还生成“训练这批样本我用多少 epoch／哪种增强／哪种超参 (learning rate etc)”等。也就是说，**训练的流程（meta-training）也在模型控制之内**
  * 这既是原论文所谓的self-edit
* 用上述自己生成的数据做sft，update weight！

至此，finetune是不是已经结束了？显然不是，可能还有如下问题：

* model自己生成的答案是不是对的？没有人工检查，谁能确保答案一定正确了？

2.3  downstream task 验证

    上一步生成的candidated self-edit答案有对有错，用来sft的效果好不好了？是骡子是马，拖出去遛一遛不就知道了？直接来硬的：**用下游任务的回答来判断上一步finetune的效果，并使用结果作为reward信号（R，self edit），继续RL,  用on-policy的策略！**

    通过 supervised finetuning (SFT) + RL outer-loop，使得模型能“持久地”适应新知识／新任务

    总结：**不只是简单粗暴地自动生成 synthetic data，而是：模型“学习如何为自己生成最有用的训练数据+训练策略”**。

* 给模型一段**新的事实**性文本，让**模型吸收该知识 (finetune后能够回答相关问题)**; 模型通过 self-edit 生成的“合成训练数据 + 微调”表现**优于直接用原始文本微调**、也**优于用 GPT-4 生成数据微调的 baselines**
* 少样本泛化 (few-shot generalization)任务上做实验：模型从**少量示例学习一个新任务，通过自己选择数据增强+训练超参，从而获得比传统 few-shot 更好的泛化**。
  * 说明self adaption在特定场景下是有效的

2.4 总结

paper的动机：LLMs are powerful but static;… they lack mechanisms to **adapt their weights in response to new tasks, knowledge, or examples**

* SEAL 提出了一个方向，向 lifelong learning 靠拢

3、其他类似不需要大量人工标注的paper：


| 方案名称                                                         | 原理（尤其是为什么不需要人工标注数据）                                                                                                                                                                                            | 使用场景                                                           | 优点                                                                            | 缺点                                                            |
| ---------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| SEAL<br /> (Self-Adapting Language Models)                       | 通过self-edits生成合成数据+训练策略（如QA对、lr、epochs），<br />用下游任务性能作为自生成reward的RL外环迭代优化。<br />为什么不需要人工标注：下游任务（如QA准确率）作为内在reward信号，<br />自评效果，无需人类标签验证edit质量。 | 知识整合（如新事实吸收）、<br />few-shot泛化（如少样本任务学习）。 | 数据高效、支持终身学习、<br />优于GPT-4合成数据基线。                           | 计算开销高（每edit评估30-45秒）、<br />潜在模式崩溃和遗忘风险。 |
| EVOL-RL <br />(Evolving Language Models without Labels)          | 进化RL框架，受生物进化启发：**多数投票（选择稳定解）+新颖奖励（促进变异探索）**。<br />为什么不需要人工标注：用无标签数据，**自一致性投票和语义新颖度作为reward信号，避免崩溃**。                                               | 数学任务（如AIME、MATH）、推理优化。                               | 稳定迭代（避免认知崩溃）、<br />显著提升准确率<br />（如pass@1从4.6%到16.4%）。 | 计算密集（需vLLM服务）、<br />焦点限于结构化任务。              |
| Large Language Models Can Self-Improve                           | 用预训练LLM生成高信心样本和理由，自微调迭代。<br />为什么不需要人工标注：内在置信度过滤生成标签，无需外部验证。                                                                                                                   | 推理任务（如GSM8K数学）、一般自改进。                              | 简单高效、零标签提升性能<br />（准确率升10-15%）。                              | 易模式崩溃、依赖基础模型质量。                                  |
| R-Zero                                                           | 两个模型互为老师/学生，自生成课程从简单到复杂进化。<br />为什么不需要人工标注：互反馈取代人类标签，用执行结果自评。                                                                                                               | 领域适应（如代码生成）、课程学习。                                 | 鲁棒、避免单一偏置。                                                            | 双模型计算开销高、初始化敏感。                                  |
| ALMA <br />(Alignment with Minimal Annotation)                   | 用基础LLM内在知识生成合成对齐数据（如偏好对）。<br />为什么不需要人工标注：预训练先验自生成标签，仅需少量种子（趋近零）。                                                                                                         | 对齐任务（如偏好优化）、一般自适应。                               | 成本低、易扩展。                                                                | 依赖基础模型、对齐质量不稳定。                                  |
| TTRL<br /> (Test-Time Reinforcement Learning)                    | 测试时RL优化推理路径，用预训练先验作为reward。<br />为什么不需要人工标注：自一致性信号取代标签，无需训练标签。                                                                                                                    | 推理优化（如MATH任务）、在线适应。                                 | 在线高效、提升pass@1 5-10%。                                                    | 易认知崩溃（需改进如EVOL-RL）。                                 |
| Self-Rewarding Language Models                                   | 模型自生成reward判断输出质量，RL迭代。<br />为什么不需要人工标注：自评机制取代人类反馈，用内在判断。                                                                                                                              | 复杂任务（如多步推理）、自对齐。                                   | 完全自主、优于无reward基线。                                                    | Reward主观、可能引入偏置。                                      |
| ICM <br />(Internal Coherence Maximization)                      | RL最大化内部一致性作为reward。<br />为什么不需要人工标注：自一致性信号，无外部标签。                                                                                                                                              | 鲁棒性提升、一般自微调。                                           | 稳定、易实现。                                                                  | 泛化有限、reward单一。                                          |
| ALAS<br /> (Autonomous Learning Agent System)                    | 代理管道（检索-生成-验证）自生成反馈循环。<br />为什么不需要人工标注：执行结果作为自然反馈，最小人类监督。                                                                                                                        | 终身学习、新领域适应。                                             | 灵活、多步交互。                                                                | 代理复杂、设计难度高。                                          |
| ACE<br /> (Evolving Contexts for Self-Improving Language Models) | 代理用执行反馈进化上下文。<br />为什么不需要人工标注：任务成功率作为自反馈，无标签监督。                                                                                                                                          | 新环境适应（如AppWorld基准）。                                     | 实用、迭代快速。                                                                | 需环境模拟、泛化依赖反馈质量。                                  |
| Autonomous Learning for Domain Adaptation                        | 代理在无标签领域数据上自创建改进环境。<br />为什么不需要人工标注：内在领域信号自生成标签。                                                                                                                                        | 领域特定任务（如跨领域转移）。                                     | 针对性强、零人类。                                                              | 领域依赖、初始化挑战。                                          |
| Self-Improving Alignment with LLM-as-a-Meta-Judge                | 模型判断自己的判断（meta-rewarding），自炼化对齐。<br />为什么不需要人工标注：自反馈循环，用元级评估。                                                                                                                            | 判断能力改进、对齐迭代。                                           | 元级优化、深度自适应。                                                          | 计算重、可能循环偏置。                                          |
| MIT Self-Trained Entailment Models                               | 无人类标签自训练蕴涵模型，用生成标签迭代。<br />为什么不需要人工标注：自生成标签，优于监督版本。                                                                                                                                  | 蕴涵任务（如逻辑推理）、模型扩展。                                 | 高效、可扩展到350M参数。                                                        | 任务特定、泛化需调优。                                          |
| Active Learning with HITL (Human-in-the-Loop)                    | 用不确定性采样最小化标注，只标注高价值样本。<br />为什么不需要人工标注：智能选择，标注量降90%（非零，但极少）。                                                                                                                   | 过渡场景（如需少量人类验证的任务）。                               | 实用、结合人类与自动化。                                                        | 非零标注、依赖采样准确性。                                      |
