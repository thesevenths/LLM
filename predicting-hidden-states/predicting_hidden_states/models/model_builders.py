from modules.architectures import LSTMPHi, TransformerDecoderPHi
from models.component_builders import lstm_phi, llama3_phi


def lstm_ascii_self_prediction(embed_dim=768,
                               num_layers=12,
                               detach_hidden_states=False,
                               self_prediction_layer=30,
                               prediction_from_z=False,
                               use_self_attention: bool = False,
                               use_self_prediction=True,
                               use_invertible_transform=False,
                               kl_div_loss_factor=0.,
                               self_critic_loss_factor=0.,
                               phi_loss_factor=0.1,
                               orthogonality_loss_factor=1.,
                               previous_hidden_loss_factor=0.,
                               detach_targets=False,
                               chance_to_deterministic=0.,
                               deterministic_at_inference=False,
                               straight_through_eval=False,
                               self_prediction_num_layers=2, ) -> LSTMPHi:
    return lstm_phi(
        vocab_size=128,
        num_layers=num_layers,
        embed_dim=embed_dim,
        max_seq_len=2048,
        intermediate_dim=embed_dim * 8 // 3,
        norm_eps=1e-5,
        tied_embeddings=True,
        use_self_attention=use_self_attention,
        self_critic_loss_factor=self_critic_loss_factor,
        phi_loss_factor=phi_loss_factor,
        detach_hidden_states=detach_hidden_states,
        use_self_prediction=use_self_prediction,
        self_prediction_layer_position=self_prediction_layer,
        self_prediction_num_layers=self_prediction_num_layers,
        detach_targets=detach_targets,
        chance_to_deterministic=chance_to_deterministic,
        deterministic_at_inference=deterministic_at_inference,
        straight_through_eval=straight_through_eval,
    )


def llama3_ascii_self_prediction_0_1b(use_self_prediction=True,
                                      detach_hidden_states=False,
                                      self_prediction_layer=30,
                                      use_self_attention: bool = False,
                                      self_critic_loss_factor=0.,
                                      phi_loss_factor=0.1,
                                      detach_targets=False,
                                      chance_to_deterministic=0.,
                                      deterministic_at_inference=False,
                                      straight_through_eval=False,
                                      self_prediction_num_layers=2,
                                      max_seq_len=2048,
                                      use_hidden_state_prediction=True,
                                      use_information_bottleneck=True,
                                      ) -> TransformerDecoderPHi:
    """
    Config approximated from GPT2-small
    """
    return llama3_phi(
        vocab_size=128,
        num_layers=12,
        num_heads=6,
        num_kv_heads=6,
        embed_dim=768,
        max_seq_len=max_seq_len,
        intermediate_dim=768 * 8 // 3,
        attn_dropout=0.0,
        norm_eps=1e-5,
        rope_base=500_000,
        tied_embeddings=True,
        use_self_attention=use_self_attention,
        self_critic_loss_factor=self_critic_loss_factor,
        phi_loss_factor=phi_loss_factor,
        detach_hidden_states=detach_hidden_states,
        use_self_prediction=use_self_prediction,
        self_prediction_layer_position=self_prediction_layer,
        self_prediction_num_layers=self_prediction_num_layers,
        detach_targets=detach_targets,
        chance_to_deterministic=chance_to_deterministic,
        deterministic_at_inference=deterministic_at_inference,
        straight_through_eval=straight_through_eval,
        use_hidden_state_prediction=use_hidden_state_prediction,
        use_information_bottleneck=use_information_bottleneck,
    )


def llama3_2_selfprediction_3b(use_self_prediction=True,
                               detach_hidden_states=False,
                               self_prediction_layer=30,
                               use_self_attention: bool = False,
                               self_critic_loss_factor=0.,
                               phi_loss_factor=0.1,
                               detach_targets=False,
                               chance_to_deterministic=0.,
                               deterministic_at_inference=False,
                               straight_through_eval=False,
                               self_prediction_num_layers=2,
                               max_seq_len=2048,
                               use_hidden_state_prediction=True,
                               use_information_bottleneck=True,
                               ) -> TransformerDecoderPHi:
    return llama3_phi(
        vocab_size=128_256,
        num_layers=28,
        num_heads=24,
        num_kv_heads=8,
        embed_dim=3072,
        max_seq_len=max_seq_len,
        intermediate_dim=8192,
        attn_dropout=0.0,
        norm_eps=1e-5,
        rope_base=500_000,
        scale_factor=32,
        tied_embeddings=True,
        use_self_attention=use_self_attention,
        self_critic_loss_factor=self_critic_loss_factor,
        phi_loss_factor=phi_loss_factor,
        detach_hidden_states=detach_hidden_states,
        use_self_prediction=use_self_prediction,
        self_prediction_layer_position=self_prediction_layer,
        self_prediction_num_layers=self_prediction_num_layers,
        detach_targets=detach_targets,
        chance_to_deterministic=chance_to_deterministic,
        deterministic_at_inference=deterministic_at_inference,
        straight_through_eval=straight_through_eval,
        use_hidden_state_prediction=use_hidden_state_prediction,
        use_information_bottleneck=use_information_bottleneck,
    )